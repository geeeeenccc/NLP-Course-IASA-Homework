{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastparquet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-23T05:53:57.021901Z","iopub.execute_input":"2023-10-23T05:53:57.022347Z","iopub.status.idle":"2023-10-23T05:54:08.817139Z","shell.execute_reply.started":"2023-10-23T05:53:57.022317Z","shell.execute_reply":"2023-10-23T05:54:08.816297Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting fastparquet\n  Downloading fastparquet-2023.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.0.3)\nRequirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (1.23.5)\nCollecting cramjam>=2.3 (from fastparquet)\n  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2023.9.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet) (21.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->fastparquet) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\nInstalling collected packages: cramjam, fastparquet\nSuccessfully installed cramjam-2.7.0 fastparquet-2023.8.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m spacy download xx_ent_wiki_sm","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:08.818783Z","iopub.execute_input":"2023-10-23T05:54:08.819078Z","iopub.status.idle":"2023-10-23T05:54:33.371786Z","shell.execute_reply.started":"2023-10-23T05:54:08.819053Z","shell.execute_reply":"2023-10-23T05:54:33.370641Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting xx-ent-wiki-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.6.0/xx_ent_wiki_sm-3.6.0-py3-none-any.whl (11.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from xx-ent-wiki-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.66.1)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (0.1.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->xx-ent-wiki-sm==3.6.0) (2.1.3)\nInstalling collected packages: xx-ent-wiki-sm\nSuccessfully installed xx-ent-wiki-sm-3.6.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('xx_ent_wiki_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport string\nfrom typing import List\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom itertools import chain\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport nltk\nimport spacy\nimport re\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:33.373046Z","iopub.execute_input":"2023-10-23T05:54:33.373381Z","iopub.status.idle":"2023-10-23T05:54:41.536249Z","shell.execute_reply.started":"2023-10-23T05:54:33.373354Z","shell.execute_reply":"2023-10-23T05:54:41.534731Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:41.538994Z","iopub.execute_input":"2023-10-23T05:54:41.539849Z","iopub.status.idle":"2023-10-23T05:54:41.550447Z","shell.execute_reply.started":"2023-10-23T05:54:41.539816Z","shell.execute_reply":"2023-10-23T05:54:41.549410Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input\n/kaggle/input/mantis-analytics-location-detection\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preferably for proccessing the datasets we need only cpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'device {device}')","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:41.551456Z","iopub.execute_input":"2023-10-23T05:54:41.551735Z","iopub.status.idle":"2023-10-23T05:54:41.561515Z","shell.execute_reply.started":"2023-10-23T05:54:41.551709Z","shell.execute_reply":"2023-10-23T05:54:41.560456Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"device cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"project_path = '/kaggle/input/mantis-analytics-location-detection/'\n\nru_geo_path = project_path + 'ru_geo_dataset.csv'\nuk_geo_path = project_path + 'uk_geo_dataset.csv'\ntest_path = project_path + 'test.csv'","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:41.563325Z","iopub.execute_input":"2023-10-23T05:54:41.564445Z","iopub.status.idle":"2023-10-23T05:54:41.573962Z","shell.execute_reply.started":"2023-10-23T05:54:41.564416Z","shell.execute_reply":"2023-10-23T05:54:41.572607Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"uk_geo_dataset = pd.read_csv(uk_geo_path, converters={\"loc_markers\": eval})\n\ngeo_dataset = uk_geo_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:41.575607Z","iopub.execute_input":"2023-10-23T05:54:41.576082Z","iopub.status.idle":"2023-10-23T05:54:56.094398Z","shell.execute_reply.started":"2023-10-23T05:54:41.576051Z","shell.execute_reply":"2023-10-23T05:54:56.093285Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"geo_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:56.095935Z","iopub.execute_input":"2023-10-23T05:54:56.096276Z","iopub.status.idle":"2023-10-23T05:54:56.121851Z","shell.execute_reply.started":"2023-10-23T05:54:56.096244Z","shell.execute_reply":"2023-10-23T05:54:56.121184Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                      text loc_markers  \\\n0        Чим довше мають скачки тиску гіпертензією, тим...          []   \n1        А поки що починали цвісти троянди, випускники ...          []   \n2        Крім того, в 2020 р. багато експертів прогнозу...          []   \n3        Сильно сумніваюся, що ви зупините свій вибір н...          []   \n4        Цей унікальний правовий важіль утворено 1998 р...          []   \n...                                                    ...         ...   \n1009995  Траєкторія польоту цих літаків є провокуючою”,...          []   \n1009996  Якщо порівняти у відсотковому співвідношенні к...          []   \n1009997  У інфікувалися четверо жінок, наймолодшій із н...          []   \n1009998  Хочу запевнити, що створення Православної Церк...          []   \n1009999  Мінрегіон хоче по-новому проектувати дороги - ...  [(46, 53)]   \n\n        org_markers   per_markers  is_valid  \n0                []            []         0  \n1                []  [(114, 131)]         0  \n2                []            []         0  \n3                []            []         0  \n4                []            []         0  \n...             ...           ...       ...  \n1009995          []    [(56, 63)]         1  \n1009996          []            []         1  \n1009997          []            []         1  \n1009998  [(29, 56)]            []         1  \n1009999  [(56, 60)]            []         1  \n\n[1010000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>loc_markers</th>\n      <th>org_markers</th>\n      <th>per_markers</th>\n      <th>is_valid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Чим довше мають скачки тиску гіпертензією, тим...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>А поки що починали цвісти троянди, випускники ...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[(114, 131)]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Крім того, в 2020 р. багато експертів прогнозу...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Сильно сумніваюся, що ви зупините свій вибір н...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Цей унікальний правовий важіль утворено 1998 р...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1009995</th>\n      <td>Траєкторія польоту цих літаків є провокуючою”,...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[(56, 63)]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1009996</th>\n      <td>Якщо порівняти у відсотковому співвідношенні к...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1009997</th>\n      <td>У інфікувалися четверо жінок, наймолодшій із н...</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1009998</th>\n      <td>Хочу запевнити, що створення Православної Церк...</td>\n      <td>[]</td>\n      <td>[(29, 56)]</td>\n      <td>[]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1009999</th>\n      <td>Мінрегіон хоче по-новому проектувати дороги - ...</td>\n      <td>[(46, 53)]</td>\n      <td>[(56, 60)]</td>\n      <td>[]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1010000 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"nlp = spacy.load(\"xx_ent_wiki_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:56.123045Z","iopub.execute_input":"2023-10-23T05:54:56.123580Z","iopub.status.idle":"2023-10-23T05:54:57.982354Z","shell.execute_reply.started":"2023-10-23T05:54:56.123550Z","shell.execute_reply":"2023-10-23T05:54:57.981292Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# A bit of an update of a code provided in the lecture.\n\n# This modified code generates BIO labels for location markers. \n# It processes the text and location markers to determine wheach token is a part of a location entity (Begin or Inside) or not (Outside). \n# We will then store the labels in a seperate column for further modelling.\n\ndef batch_bio_labeling(texts, loc_markers_list, tokenizer, batch_size=128, n_process=16, verbose=False):\n    if verbose:\n        print(\"Applying Tokenizer\")\n    docs = list(tokenizer.pipe(texts, batch_size=batch_size, n_process=n_process))\n    batch_results = []\n\n    if verbose:\n        print(\"Extracting BIO labels\")\n    for doc, loc_markers in zip(docs, loc_markers_list):\n        tokens = [token.text for token in doc]\n        labels = ['O'] * len(tokens)\n        \n        for start, end in loc_markers:\n            # Find the token indexes that correspond to the entity's start and end positions\n            start_idx = max(0, next((i for i, token in enumerate(doc) if token.idx >= start), -1))\n            end_idx = min(len(doc), next((i for i, token in enumerate(doc) if token.idx + len(token.text) >= end), 999))\n\n            if start_idx is not None and end_idx is not None:\n                # Mark the first token as B-LOC (beginning) and the rest as I-LOC (inside)\n                labels[start_idx] = 'B-LOC'\n                for i in range(start_idx + 1, end_idx):\n                    labels[i] = 'I-LOC'\n\n        batch_results.append((tokens, labels))\n                \n    return batch_results\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:57.984593Z","iopub.execute_input":"2023-10-23T05:54:57.985080Z","iopub.status.idle":"2023-10-23T05:54:57.993837Z","shell.execute_reply.started":"2023-10-23T05:54:57.985052Z","shell.execute_reply":"2023-10-23T05:54:57.992444Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"geo_processed_dataset = batch_bio_labeling(geo_dataset.text.to_list(), geo_dataset.loc_markers.to_list(), nlp, verbose=True)\n\ngeo_processed_df = pd.DataFrame({\n    \"tokens\": [el[0] for el in geo_processed_dataset],\n    \"labels\": [el[1] for el in geo_processed_dataset],\n    \"is_valid\": geo_dataset[\"is_valid\"].to_list()\n})\n\n# Let's save processed dataset if we need it in further experiments or just for back-up :)\ngeo_processed_df.to_parquet(\n    'uk_geo_processed.parquet', \n    engine='fastparquet'\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T05:54:57.995418Z","iopub.execute_input":"2023-10-23T05:54:57.995730Z","iopub.status.idle":"2023-10-23T06:21:05.988454Z","shell.execute_reply.started":"2023-10-23T05:54:57.995706Z","shell.execute_reply":"2023-10-23T06:21:05.987337Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Applying Tokenizer\nExtracting BIO labels\n","output_type":"stream"}]},{"cell_type":"code","source":"ru_geo_dataset = pd.read_csv(ru_geo_path, converters={\"loc_markers\": eval})\n\ngeo_dataset = ru_geo_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-22T23:10:11.489540Z","iopub.execute_input":"2023-10-22T23:10:11.489988Z","iopub.status.idle":"2023-10-22T23:10:34.618967Z","shell.execute_reply.started":"2023-10-22T23:10:11.489948Z","shell.execute_reply":"2023-10-22T23:10:34.617100Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ru_geo_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mru_geo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloc_markers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m geo_dataset \u001b[38;5;241m=\u001b[39m ru_geo_dataset\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1015\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2084\u001b[0m, in \u001b[0;36mpandas._libs.parsers._apply_converter\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m<string>:1\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"geo_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-22T23:10:34.619982Z","iopub.status.idle":"2023-10-22T23:10:34.620421Z","shell.execute_reply.started":"2023-10-22T23:10:34.620231Z","shell.execute_reply":"2023-10-22T23:10:34.620251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# For some reason kaggle didn't really like me trying to procces the whole ru dataset\n# (it was killing the kernel every time, \n# saying that I had been reaching some limit of data storing in kaggle notebook or whatever)\n# So I had to break the ru dataset in 9 smaller parts\n# This allows us to have some back-up data in case \n\nnum_parts = 9\npart_size = len(geo_dataset) // num_parts\n\n# Initialize an empty DataFrame to store the combined data\ncombined_df = pd.DataFrame()\n\nfor part in range(num_parts):\n    start_idx = part * part_size\n    end_idx = (part + 1) * part_size if part < num_parts - 1 else len(geo_dataset)\n    \n    # Extract the subset of your dataset for the current part\n    geo_subset = geo_dataset[start_idx:end_idx]\n\n    print(f'part_{part+1}')\n    print(len(geo_subset))\n    geo_processed_dataset = batch_bio_labeling(geo_subset.text.to_list(), geo_subset.loc_markers.to_list(), nlp, verbose=True)\n    \n    # Convert the processed dataset to a DataFrame\n    geo_processed_df = pd.DataFrame({\n        \"tokens\": [el[0] for el in geo_processed_dataset],\n        \"labels\": [el[1] for el in geo_processed_dataset],\n        \"doc_id\": geo_subset[\"doc_id\"].to_list(),\n        \"sent_id\": geo_subset[\"sent_id\"].to_list(),\n    })\n    \n    # Save the processed dataset as a parquet file\n    filename = f'ru_geo_dataset_BIO_labeled_part{part + 1}.parquet'\n    geo_processed_df.to_parquet(filename, engine='fastparquet')\n    print(f'Saved {filename}')\n    \n    combined_df = pd.concat([combined_df, geo_processed_df])\n\n\n# Save the merged dataframe as a parquet file\ncombined_df.to_parquet('ru_geo_processed.parquet', engine='fastparquet')\nprint('Saved ru_geo_processed.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-10-22T21:19:41.894946Z","iopub.execute_input":"2023-10-22T21:19:41.895284Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"part_7\n892093\nApplying Tokenizer\nExtracting BIO labels\nSaved ru_geo_dataset_BIO_labeled_part7.parquet\npart_8\n892093\n","output_type":"stream"}]}]}